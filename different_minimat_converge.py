#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Oct 24 12:34:57 2019

@author: divyarth
"""

# -*- coding: utf-8 -*-
"""ASSIGNEMENT3_DIVY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hnOA2FMEnWNCO874DrwJXZMQsFVlJtfz
"""
import sys

import numpy as np
import scipy as sp
import pandas as pd
from numpy.random import seed


from tensorflow import set_random_seed
seed(1)
set_random_seed(1)


#sys.argv=['','/content/drive/My Drive/Colab Notebooks/train.csv','/content/drive/My Drive/Colab Notebooks/test.csv','/content/drive/My Drive/Colab Notebooks/out.txt']
data = pd.read_csv(sys.argv[1],header=None,delimiter=' ').values

X_train=data[:,:-2]
y1=data[:,-1]
y2=data[:,-2]
X_train = X_train.reshape(len(X_train),3,32,32).transpose([0,2, 3, 1])/255


data1 = pd.read_csv(sys.argv[2],header=None,delimiter=' ').values
X_test=data1[:,:-2]
X_test = X_test.reshape(len(X_test),3,32,32).transpose([0,2, 3, 1])/255


from keras.utils import to_categorical
y_train = to_categorical(y1)
from keras.models import Model
from keras.layers import Conv2D, MaxPooling2D, Dense, Input, Activation, Dropout, GlobalAveragePooling2D,\
    BatchNormalization, concatenate, AveragePooling2D
from keras.optimizers import Adam
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.callbacks import EarlyStopping

def make_model():
  model = Sequential()
  
  model.add(Conv2D(128,(3,3),activation='elu',input_shape=(32,32,3),padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(128,(1,1),activation='elu',padding='same'))
  model.add(Conv2D(128,(3,3),activation='elu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.1))
  model.add(BatchNormalization())

  model.add(Conv2D(256,(3,3),activation='elu',padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(256,(1,1),activation='elu',padding='same'))
  model.add(Conv2D(256,(3,3),activation='elu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.25))
  model.add(BatchNormalization())

  model.add(Conv2D(512,(3,3),activation='elu',padding='same'))
  model.add(BatchNormalization()) 
  model.add(Conv2D(512,(1,1),activation='elu',padding='same'))
  model.add(Conv2D(512,(3,3),activation='elu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())

  model.add(GlobalAveragePooling2D())

  model.add(Dense(1024,activation='elu'))
  model.add(Dropout(0.50))
  model.add(BatchNormalization())

  model.add(Dense(100,activation='softmax'))
  return model



from keras.callbacks import EarlyStopping
es=EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
from keras.utils import to_categorical

y_train = to_categorical(y1)
md1=make_model()
md1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
es=EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
history1 = md1.fit(X_train, y_train, validation_split =0.1, epochs=15, batch_size=1000,callbacks=[es])

md2=make_model()
md2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
es=EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
history2 = md2.fit(X_train, y_train, validation_split =0.1, epochs=15, batch_size=200,callbacks=[es])

md3=make_model()
md3.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
es=EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
history3 = md3.fit(X_train, y_train, validation_split =0.1, epochs=15, batch_size=500,callbacks=[es])

md4=make_model()
md4.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
es=EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
history1 = md4.fit(X_train, y_train, validation_split =0.1, epochs=15, batch_size=100,callbacks=[es])

y_p1 = md1.predict(X_test)

y_p2 = md2.predict(X_test)

y_p3=md3.predict(X_test)

y_p4=md4.predict(X_test)

y_test=(y_p4+y_p1+y_p2+y_p3).argmax(axis=1)

np.savetxt(sys.argv[3],y_test,delimiter='\n')

